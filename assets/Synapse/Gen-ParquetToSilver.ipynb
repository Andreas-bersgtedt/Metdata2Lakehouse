{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Parameters\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter cell\r\n",
        "ListOfTables = \"{'name':'productmodelproductdescription','partitionkey':'ProductModelID','uuid':'F7943D78-ECFD-4246-81A6-C8221F43BDBB','PK_COLS':'ProductDescriptionID'}\"\r\n",
        "bronze_format = 'parquet'\r\n",
        "datasource = 'wwi'\r\n",
        "bronze_date_slize = \"2023/07/17/16/\"\r\n",
        "bronze_account_host_name = \"{storageAccount}.dfs.core.windows.net\"\r\n",
        "bronze_container_name = \"bronze\"\r\n",
        "silver_account_host_name = \"{storageAccount}.dfs.core.windows.net\"\r\n",
        "silver_container_name = \"silver\"\r\n",
        "bronze_relative_path = \"salesdata/saleslt\" \r\n",
        "silver_relative_path = \"salesdata/saleslt\"\r\n",
        "target_mode ='RL'\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Documentation #\r\n",
        "\r\n",
        "## *ListOfTables:*\r\n",
        "This is the main attribute list of tables that needs to be processed in the main for each loop, this is constructed as:\r\n",
        "{'name':'***Table Name***','partitionkey':'***Column that the table is partitioned by***','uuid':'***This is the GUID for the source extraction log***','PK_COLS':'***This is the primary key from source***'}\r\n",
        "## *bronze_format* \r\n",
        "This controls the spark.read format when loading the source data, we assume this is  ***parquet***\r\n",
        "## *datasource*\r\n",
        "This is unused\r\n",
        "## *bronze_date_slize*\r\n",
        "This is the date folder in the bronze folder ie: ***\"YYYY/MM/DD/hh/\"***\r\n",
        "## *bronze_account_host_name*\r\n",
        "This is the bronze storage account FQDN IE: \"<Hostname>.dfs.core.windows.net\"\r\n",
        "## *bronze_container_name*\r\n",
        "This is the storage container name.\r\n",
        "## *silver_account_host_name*\r\n",
        " This is the silver storage account FQDN IE: \"<Hostname>.dfs.core.windows.net\"\r\n",
        "## *silver_container_name*\r\n",
        "This is the storage container name.\r\n",
        "## *bronze_relative_path*\r\n",
        "This is the base folder path of the source data batch ie: \"***folder/sub_folder***\" \r\n",
        "## *silver_relative_path* \r\n",
        "This is the base folder path of the target table batch ie: \"***folder/sub_folder***\"\r\n",
        "## *target_mode*\r\n",
        "This defines how to handle the batch IE: is the transaction append/incremental insert(***II***) , Reload table(***RL***), Merge Insert(***MI***) or Upsert(***US***)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Microsoft Corporation.\r\n",
        "# Licensed under the MIT License.\r\n",
        "# Spark session configuration\r\n",
        "spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\r\n",
        "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n",
        "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\r\n",
        "\r\n",
        "# Runtime variables\r\n",
        "import ast\r\n",
        "bronze_layer = 'abfss://'+bronze_container_name+'@'+bronze_account_host_name+'/'+bronze_relative_path+'/'\r\n",
        "silver_layer = 'abfss://'+silver_container_name+'@'+silver_account_host_name+'/'+silver_relative_path\r\n",
        "\r\n",
        "print(bronze_layer)\r\n",
        "print(silver_layer)\r\n",
        "\r\n",
        "ListOfTables = \"[\" + ListOfTables + \"]\"\r\n",
        "# Parse the string as a Python expression and get a list of dictionaries\r\n",
        "mylist = ast.literal_eval(ListOfTables)\r\n",
        "#print(ListOfTables)\r\n",
        "\r\n",
        "tables = mylist\r\n",
        "#print(tables)\r\n"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0196844Z\",\"session_start_time\":\"2023-04-14T17:53:12.2618501Z\",\"execution_start_time\":\"2023-04-14T17:53:22.8437076Z\",\"execution_finish_time\":\"2023-04-14T17:53:24.8678387Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurse over source tables\r\n",
        "This cell creates a function to read raw data from the _Bronze_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from delta.tables import *\r\n",
        "\r\n",
        "\r\n",
        "#build UDF execution pattern for the For Loop\r\n",
        "def loadFullDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n",
        "    #state = '1'\r\n",
        "    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n",
        "    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n",
        "    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n",
        "    print(table_name)\r\n",
        "    if len(partition_by_clause) > 1:\r\n",
        "        df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n",
        "        \r\n",
        "    else:\r\n",
        "        df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n",
        "        \r\n",
        "#build UDF execution pattern for the For Loop\r\n",
        "def loadIncrementalInsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n",
        "    #state = '1'\r\n",
        "    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n",
        "    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n",
        "    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n",
        "    print(table_name)\r\n",
        "    if len(partition_by_clause) > 1:\r\n",
        "        df.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n",
        "      \r\n",
        "    else:\r\n",
        "        df.write.mode(\"append\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n",
        "\r\n",
        "#build UDF execution pattern for the For Loop\r\n",
        "def loadMergeInsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n",
        "    #state = '1'\r\n",
        "    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n",
        "    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n",
        "    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n",
        "    print(table_name)\r\n",
        "    if DeltaTable.isDeltaTable(spark,  silver_layer + '/' + table_name):\r\n",
        "        print(\"The delta table exists!\")\r\n",
        "        deltaTable = DeltaTable.forPath(spark, silver_layer + '/' + table_name)\r\n",
        "        deltaTable.alias(\"t\").merge(df.alias(\"s\"),\"t.\" + businesskey + \" = s.\" + businesskey ).whenNotMatchedInsertAll().execute()\r\n",
        "    else:\r\n",
        "        print(\"The delta table does not exist.\")\r\n",
        "        if len(partition_by_clause) > 1:\r\n",
        "            df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n",
        "        \r\n",
        "        else:\r\n",
        "            df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "#build UDF execution pattern for the For Loop\r\n",
        "def loadUpsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n",
        "    #state = '1'\r\n",
        "    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n",
        "    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n",
        "    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n",
        "    print(table_name)\r\n",
        "    if DeltaTable.isDeltaTable(spark,  silver_layer + '/' + table_name):\r\n",
        "        print(\"The delta table exists!\")\r\n",
        "        deltaTable = DeltaTable.forPath(spark, silver_layer + '/' + table_name)\r\n",
        "        deltaTable.alias(\"t\").merge(df.alias(\"s\"),\"t.\" + businesskey + \" = s.\" + businesskey ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
        "    else:\r\n",
        "        print(\"The delta table does not exist.\")\r\n",
        "        if len(partition_by_clause) > 1:\r\n",
        "            df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n",
        "        \r\n",
        "        else:\r\n",
        "            df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "full_tables = tables\r\n",
        "for item in tables:\r\n",
        "    name, partitionkey, uuid, PK_COLS = item.values()\r\n",
        "    if target_mode =='RL': \r\n",
        "        loadFullDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n",
        "    if target_mode =='II': \r\n",
        "        loadIncrementalInsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n",
        "    if target_mode =='MI': \r\n",
        "        loadMergeInsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n",
        "    if target_mode =='US': \r\n",
        "        loadUpsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n",
        "    else:\r\n",
        "        print(\"Nothing to do...\")\r\n"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0270275Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T17:56:22.4495972Z\",\"execution_finish_time\":\"2023-04-14T17:56:32.6772207Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": "Generic PySpark Notebook that reads Parquet data sets and converts them into delta tables in silver Zone",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}