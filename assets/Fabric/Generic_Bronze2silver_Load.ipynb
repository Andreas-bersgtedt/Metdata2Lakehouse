{"cells":[{"cell_type":"code","source":["# parameter cell\r\n","ListOfTables = \"{'name':'productmodelproductdescription','partitionkey':'ProductModelID','uuid':'F7943D78-ECFD-4246-81A6-C8221F43BDBB','PK_COLS':'ProductDescriptionID'},{'name':'salesorderdetail','partitionkey':'','uuid':'5DF6AB3A-B306-45D7-B63F-A3BC3B92466B','PK_COLS':''},{'name':'salesorderheader','partitionkey':'','uuid':'BB74FEA8-C9D0-4B71-8F6B-E1185585C511','PK_COLS':''},{'name':'vgetallcategories','partitionkey':'','uuid':'2B84F980-33E8-4FDA-A560-1B0C4741ECEC','PK_COLS':''}\"\r\n","bronze_format = 'parquet'\r\n","datasource = 'wwi'\r\n","bronze_date_slize = \"2023/07/17/16/\"\r\n","bronze_container_name = \"bronze\"\r\n","bronze_relative_path = \"salesdata/saleslt\" \r\n","silver_relative_path = \"salesdata/saleslt\"\r\n","target_mode ='RL'\r\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"c303e51e-377d-4b38-961e-fca82133970b"},{"cell_type":"markdown","source":["# Parameter Documentation #\r\n","\r\n","## *ListOfTables:*\r\n","This is the main attribute list of tables that needs to be processed in the main for each loop, this is constructed as:\r\n","{'name':'***Table Name***','partitionkey':'***Column that the table is partitioned by***','uuid':'***This is the GUID for the source extraction log***','PK_COLS':'***This is the primary key from source***'}\r\n","## *bronze_format* \r\n","This controls the spark.read format when loading the source data, we assume this is  ***parquet***\r\n","## *datasource*\r\n","This is unused\r\n","## *bronze_date_slize*\r\n","This is the date folder in the bronze folder ie: ***\"YYYY/MM/DD/hh/\"***\r\n","## *bronze_container_name*\r\n","This is the storage container name.\r\n","## *silver_container_name*\r\n","This is the storage container name.\r\n","## *bronze_relative_path*\r\n","This is the base folder path of the source data batch ie: \"***folder/sub_folder***\" \r\n","## *silver_relative_path* \r\n","This is the base folder path of the target table batch ie: \"***folder/sub_folder***\"\r\n","## *target_mode*\r\n","This defines how to handle the batch IE: is the transaction append/incremental insert(***II***) , Reload table(***RL***), Merge Insert(***MI***) or Upsert(***US***)\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7fc6cc62-876f-4d7d-885d-353c3f7d3020"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation.\n","# Licensed under the MIT License.\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[],"execution_count":null,"metadata":{},"id":"d6af16cf-8181-4167-a104-5fe5a472ac47"},{"cell_type":"code","source":["# Runtime variables\r\n","import ast\r\n","bronze_layer = 'Files/bronze/'+bronze_relative_path+'/'\r\n","silver_layer = 'Tables/'\r\n","silver_relative_path = silver_relative_path.replace('/','_')\r\n","print(bronze_layer)\r\n","print(silver_layer)\r\n","\r\n","ListOfTables = \"[\" + ListOfTables + \"]\"\r\n","# Parse the string as a Python expression and get a list of dictionaries\r\n","mylist = ast.literal_eval(ListOfTables)\r\n","#print(ListOfTables)\r\n","print(silver_relative_path)\r\n","tables = mylist\r\n","#print(tables)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6d81d199-c9fb-4d3c-9010-404b643924a3"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","from pyspark.sql.functions import *\r\n","from delta.tables import *\r\n","\r\n","\r\n","#build UDF execution pattern for the For Loop\r\n","def loadFullDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n","    #state = '1'\r\n","    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n","    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n","    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n","    print(table_name)\r\n","    if len(partition_by_clause) > 1:\r\n","        df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n","        \r\n","    else:\r\n","        df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n","        \r\n","#build UDF execution pattern for the For Loop\r\n","def loadIncrementalInsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n","    #state = '1'\r\n","    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n","    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n","    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n","    print(table_name)\r\n","    if len(partition_by_clause) > 1:\r\n","        df.write.mode(\"append\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n","      \r\n","    else:\r\n","        df.write.mode(\"append\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n","\r\n","#build UDF execution pattern for the For Loop\r\n","def loadMergeInsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n","    #state = '1'\r\n","    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n","    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n","    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n","    print(table_name)\r\n","    if DeltaTable.isDeltaTable(spark,  silver_layer + '/' + table_name):\r\n","        print(\"The delta table exists!\")\r\n","        deltaTable = DeltaTable.forPath(spark, silver_layer + '/' + table_name)\r\n","        deltaTable.alias(\"t\").merge(df.alias(\"s\"),\"t.\" + businesskey + \" = s.\" + businesskey ).whenNotMatchedInsertAll().execute()\r\n","    else:\r\n","        print(\"The delta table does not exist.\")\r\n","        if len(partition_by_clause) > 1:\r\n","            df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n","        \r\n","        else:\r\n","            df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n","    \r\n","    \r\n","    \r\n","\r\n","#build UDF execution pattern for the For Loop\r\n","def loadUpsertDataFromSource(table_name,partition_by_clause,log_uuid,businesskey):\r\n","    #state = '1'\r\n","    df = spark.read.format(bronze_format).load(bronze_layer + table_name + '/' + bronze_date_slize)\r\n","    df = df.withColumn('trans_log_id',lit(log_uuid))\r\n","    df = df.withColumn(\"trans_utc_timestamp\", lit(current_timestamp()))\r\n","    print(table_name)\r\n","    if DeltaTable.isDeltaTable(spark,  silver_layer + '/' + table_name):\r\n","        print(\"The delta table exists!\")\r\n","        deltaTable = DeltaTable.forPath(spark, silver_layer + '/' + table_name)\r\n","        deltaTable.alias(\"t\").merge(df.alias(\"s\"),\"t.\" + businesskey + \" = s.\" + businesskey ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n","    else:\r\n","        print(\"The delta table does not exist.\")\r\n","        if len(partition_by_clause) > 1:\r\n","            df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(partition_by_clause).save(silver_layer + '/' + table_name)\r\n","        \r\n","        else:\r\n","            df.write.mode(\"overwrite\").format(\"delta\").save(silver_layer + '/' + table_name)\r\n","\r\n","\r\n","\r\n","\r\n","full_tables = tables\r\n","for item in tables:\r\n","    name, partitionkey, uuid, PK_COLS = item.values()\r\n","    if target_mode =='RL': \r\n","        loadFullDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n","    if target_mode =='II': \r\n","        loadIncrementalInsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n","    if target_mode =='MI': \r\n","        loadMergeInsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n","    if target_mode =='US': \r\n","        loadUpsertDataFromSource(name, partitionkey, uuid, PK_COLS)\r\n","    else:\r\n","        print(\"Nothing to do...\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"10effe1a-5fae-4096-ad14-b60dc6dc5e11"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":null,"known_lakehouses":[]}}},"nbformat":4,"nbformat_minor":5}